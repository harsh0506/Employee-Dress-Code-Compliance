{"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader, random_split\n","from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import os\n","\n","# Additional imports for ROC, AUC, and Confusion Matrix\n","from sklearn.preprocessing import label_binarize\n","from itertools import cycle\n","\n","# Combined ResNet and MobileNet model with Attention, Dropout, and Regularization\n","class CombinedModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CombinedModel, self).__init__()\n","\n","        # Load pre-trained ResNet (from torchvision)\n","        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n","        self.resnet_features = nn.Sequential(*list(self.resnet.children())[:-2])  # Remove final FC and AvgPool layer\n","\n","        # Load pre-trained MobileNetV3 from torchvision\n","        self.mobilenet = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n","        self.mobilenet_features = nn.Sequential(*list(self.mobilenet.children())[:-1])  # Remove classifier\n","\n","        # Global Average Pooling (GAP) after feature extraction\n","        self.gap = nn.AdaptiveAvgPool2d(1)  # Adaptive pooling to (1, 1)\n","\n","        # Calculate the actual combined feature size dynamically based on the models\n","        self.resnet_feature_size = self.resnet.fc.in_features  # ResNet's last layer in_features size\n","        self.mobilenet_feature_size = self.mobilenet.classifier[0].in_features  # MobileNet's classifier in_features\n","        self.combined_feature_size = self.resnet_feature_size + self.mobilenet_feature_size\n","\n","        # Attention layer for combined features\n","        self.attention = AttentionLayer(self.combined_feature_size)\n","\n","        # Classification head (with dropout to avoid overfitting)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(self.combined_feature_size, 1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),  # Increase dropout for regularization\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),  # Another dropout layer\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        # Extract features from ResNet\n","        resnet_features = self.resnet_features(x)\n","        resnet_features = self.gap(resnet_features)  # Apply GAP\n","        resnet_features = resnet_features.view(resnet_features.size(0), -1)\n","\n","        # Extract features from MobileNet\n","        mobilenet_features = self.mobilenet_features(x)\n","        mobilenet_features = self.gap(mobilenet_features)  # Apply GAP\n","        mobilenet_features = mobilenet_features.view(mobilenet_features.size(0), -1)\n","\n","        # Concatenate the feature maps\n","        combined_features = torch.cat((resnet_features, mobilenet_features), dim=1)\n","\n","        # Pass through attention mechanism\n","        combined_features = self.attention(combined_features)\n","\n","        # Classification head\n","        out = self.classifier(combined_features)\n","        return out\n","\n","# Data Augmentation: Enhanced with random cropping, color jitter, etc.\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(224),  # Random crop and resize to 224x224\n","    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),  # Random color jitter\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize for consistency\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","# Load dataset using ImageFolder\n","data_dir = '/content/drive/MyDrive/clothing_data'\n","dataset = datasets.ImageFolder(root=data_dir, transform=train_transform)\n","\n","# Split the dataset into train and test sets\n","train_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_size\n","train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","# DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Model Checkpointing (save the model with the lowest validation loss)\n","def save_model(model, optimizer, epoch, loss, path='best_model.pth'):\n","    state = {\n","        'model': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","        'epoch': epoch,\n","        'loss': loss\n","    }\n","    torch.save(state, path)\n","\n","# Training and Evaluation Functions\n","def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=10):\n","    best_loss = float('inf')\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for images, labels in train_loader:\n","            images, labels = images.cuda(), labels.cuda()\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        avg_train_loss = running_loss / len(train_loader)\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}')\n","\n","        # Validate model after each epoch\n","        avg_val_loss = validate_model(model, test_loader, criterion)\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}')\n","\n","        # Save the model with the lowest validation loss\n","        if avg_val_loss < best_loss:\n","            best_loss = avg_val_loss\n","            save_model(model, optimizer, epoch, best_loss)\n","            print(f\"Model saved at epoch {epoch+1} with validation loss {best_loss:.4f}\")\n","\n","def validate_model(model, test_loader, criterion):\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.cuda(), labels.cuda()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","    return val_loss / len(test_loader)\n","\n","def evaluate_model(model, test_loader):\n","    model.eval()\n","    y_true = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.cuda(), labels.cuda()\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(predicted.cpu().numpy())\n","\n","    accuracy = accuracy_score(y_true, y_pred)\n","    print(f'Accuracy: {accuracy * 100:.2f}%')\n","\n","# Instantiate the model, criterion, and optimizer\n","num_classes = 8  # Adjust this based on your dataset\n","model = CombinedModel(num_classes).cuda()\n","\n","# Use CrossEntropyLoss and Adam optimizer with weight decay (L2 regularization)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n","\n","# Train and Evaluate the Model\n","train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=10)\n","\n","# Load the best saved model for final evaluation\n","checkpoint = torch.load('best_model.pth')\n","model.load_state_dict(checkpoint['model'])\n","evaluate_model(model, test_loader)\n","\n","# Evaluation function to include AUC, ROC, Classification Report, and Confusion Matrix\n","def evaluate_model(model, test_loader, num_classes):\n","    model.eval()\n","    y_true = []\n","    y_pred = []\n","    y_prob = []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.cuda(), labels.cuda()\n","\n","            outputs = model(images)\n","            probabilities = torch.softmax(outputs, dim=1)  # Get probabilities for AUC/ROC\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(predicted.cpu().numpy())\n","            y_prob.extend(probabilities.cpu().numpy())\n","\n","    # Classification Report\n","    print(\"Classification Report:\\n\")\n","    print(classification_report(y_true, y_pred))\n","\n","    # Confusion Matrix\n","    print(\"Confusion Matrix:\\n\")\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(num_classes), yticklabels=range(num_classes))\n","    plt.title('Confusion Matrix')\n","    plt.ylabel('True Label')\n","    plt.xlabel('Predicted Label')\n","    plt.show()\n","\n","    # ROC Curve and AUC for each class\n","    y_true_bin = label_binarize(y_true, classes=range(num_classes))  # Binarize labels for multi-class ROC\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","\n","    for i in range(num_classes):\n","        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], np.array(y_prob)[:, i])\n","        roc_auc[i] = roc_auc_score(y_true_bin[:, i], np.array(y_prob)[:, i])\n","\n","    # Plot ROC curve for each class\n","    plt.figure()\n","    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple', 'brown', 'pink', 'gray', 'yellow'])\n","    for i, color in zip(range(num_classes), colors):\n","        plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'Class {i} (area = {roc_auc[i]:.2f})')\n","\n","    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","    # AUC Score (micro-average)\n","    print(f'AUC (micro-average): {roc_auc_score(y_true_bin, y_prob, average=\"micro\"):.4f}')\n","\n","# Train and Validate as before (without changes)\n","\n","# Load the best saved model for final evaluation\n","checkpoint = torch.load('best_model.pth')\n","model.load_state_dict(checkpoint['model'])\n","\n","# Evaluate the model with AUC, ROC, and Confusion Matrix\n","evaluate_model(model, test_loader, num_classes=num_classes)"],"metadata":{"id":"OSuaBE8T3Nk6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qbveajb0iMl"},"outputs":[],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import MobileNetV3Small, EfficientNetV2B0, ResNet50, DenseNet121\n","from tensorflow.keras.applications.mobilenet_v3 import preprocess_input as mobilenet_preprocess\n","from tensorflow.keras.applications.efficientnet_v2 import preprocess_input as efficientnet_preprocess\n","from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n","from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.utils import class_weight\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3UBcwHwc0kTt"},"outputs":[],"source":["data_dir = \"/content/drive/MyDrive/Clothing_Data_HRP\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zn5zf9JF18HC"},"outputs":[],"source":["# Data Augmentation\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","val_datagen = ImageDataGenerator()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUOSSdQO2Anb"},"outputs":[],"source":["# Load Data and Split with class balance\n","def load_data(data_dir):\n","    data_generator = ImageDataGenerator(rescale=1./255)\n","    data_flow = data_generator.flow_from_directory(\n","        data_dir,\n","        target_size=(224, 224),\n","        batch_size=32,\n","        class_mode='categorical',\n","        shuffle=False\n","    )\n","    X, y = [], []\n","    for _ in range(len(data_flow)):\n","        x_batch, y_batch = next(data_flow)\n","        X.extend(x_batch)\n","        y.extend(y_batch)\n","    return np.array(X), np.array(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":161850,"status":"ok","timestamp":1729058202755,"user":{"displayName":"EmailForMScAILecsInLab","userId":"14358357769279669997"},"user_tz":-330},"id":"71Kxv4rU2ESk","outputId":"46f6d5ad-e2f7-4764-b18e-4e1aadc39d31"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 810 images belonging to 8 classes.\n"]}],"source":["# Split the data into train and validation sets with stratification\n","X, y = load_data(data_dir)\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1729060701243,"user":{"displayName":"EmailForMScAILecsInLab","userId":"14358357769279669997"},"user_tz":-330},"id":"dHfTU6X1DCGO","outputId":"78b816f3-2679-48c8-da30-a3222cd8c434"},"outputs":[{"data":{"text/plain":["(648, 224, 224, 3)"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ytxcdto_2fca"},"outputs":[],"source":["# Calculate class weights to handle class imbalance\n","class_weights = class_weight.compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(np.argmax(y_train, axis=1)),\n","    y=np.argmax(y_train, axis=1)\n",")\n","class_weights = dict(enumerate(class_weights))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":425,"status":"ok","timestamp":1729060789558,"user":{"displayName":"EmailForMScAILecsInLab","userId":"14358357769279669997"},"user_tz":-330},"id":"K4CqDlcBDcQD","outputId":"a1cd592c-214b-4d37-a8df-918a3d4df04f"},"outputs":[{"data":{"text/plain":["{0: 1.0125,\n"," 1: 1.0125,\n"," 2: 1.0125,\n"," 3: 1.0125,\n"," 4: 1.0125,\n"," 5: 1.0125,\n"," 6: 1.0125,\n"," 7: 0.9204545454545454}"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["class_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MnhHd7Y8nEp"},"outputs":[],"source":["# Define callbacks: EarlyStopping and ModelCheckpoint\n","early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugYH-HyY8p_a"},"outputs":[],"source":["# Function to build models\n","def build_model(model_name, preprocess_func):\n","    base_model = None\n","    if model_name == 'mobilenet_v3_small':\n","        base_model = MobileNetV3Small(weights='imagenet', input_shape=(224, 224, 3), include_top=False)\n","    elif model_name == 'efficientnet_v2_b0':\n","        base_model = EfficientNetV2B0(weights='imagenet', input_shape=(224, 224, 3), include_top=False)\n","    elif model_name == 'resnet50':\n","        base_model = ResNet50(weights='imagenet', input_shape=(224, 224, 3), include_top=False)\n","    elif model_name == 'densenet121':\n","        base_model = DenseNet121(weights='imagenet', input_shape=(224, 224, 3), include_top=False)\n","\n","    base_model.trainable = False\n","\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n","        tf.keras.layers.Lambda(preprocess_func),\n","        base_model,\n","        tf.keras.layers.GlobalAveragePooling2D(),\n","        tf.keras.layers.Dense(512, activation='relu'),\n","        tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(len(np.unique(np.argmax(y_train, axis=1))), activation='softmax')\n","    ])\n","\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbA7AwCd-Idk"},"outputs":[],"source":["# Model List\n","models = {\n","    'mobilenet_v3_small': mobilenet_preprocess,\n","    'efficientnet_v2_b0': efficientnet_preprocess,\n","    'resnet50': resnet_preprocess,\n","    'densenet121': densenet_preprocess\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":865600,"status":"ok","timestamp":1729060287423,"user":{"displayName":"EmailForMScAILecsInLab","userId":"14358357769279669997"},"user_tz":-330},"id":"OuKTQx3v-Lgx","outputId":"4ae971f0-582c-4566-9832-1e6edf8f7a91"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training mobilenet_v3_small...\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top_v2.h5\n","\u001b[1m4334752/4334752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.1103 - loss: 2.1735 - val_accuracy: 0.1049 - val_loss: 2.1222\n","Epoch 2/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 218ms/step - accuracy: 0.1239 - loss: 2.1600 - val_accuracy: 0.1049 - val_loss: 2.0833\n","Epoch 3/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 212ms/step - accuracy: 0.1050 - loss: 2.1277 - val_accuracy: 0.1235 - val_loss: 2.0897\n","Epoch 4/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 299ms/step - accuracy: 0.1356 - loss: 2.0995 - val_accuracy: 0.1235 - val_loss: 2.0905\n","Epoch 5/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 376ms/step - accuracy: 0.1301 - loss: 2.1045 - val_accuracy: 0.1296 - val_loss: 2.0809\n","Epoch 6/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 242ms/step - accuracy: 0.1272 - loss: 2.1057 - val_accuracy: 0.1358 - val_loss: 2.0802\n","Epoch 7/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 317ms/step - accuracy: 0.1283 - loss: 2.0990 - val_accuracy: 0.1235 - val_loss: 2.0824\n","Epoch 8/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 199ms/step - accuracy: 0.1460 - loss: 2.0764 - val_accuracy: 0.1235 - val_loss: 2.0811\n","Epoch 9/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 275ms/step - accuracy: 0.1316 - loss: 2.0984 - val_accuracy: 0.1111 - val_loss: 2.0801\n","Epoch 10/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 216ms/step - accuracy: 0.1187 - loss: 2.0900 - val_accuracy: 0.1049 - val_loss: 2.0798\n","Epoch 11/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 240ms/step - accuracy: 0.1525 - loss: 2.0816 - val_accuracy: 0.1358 - val_loss: 2.0791\n","Epoch 12/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 294ms/step - accuracy: 0.1203 - loss: 2.0805 - val_accuracy: 0.1358 - val_loss: 2.0792\n","Epoch 13/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 200ms/step - accuracy: 0.1054 - loss: 2.0824 - val_accuracy: 0.1358 - val_loss: 2.0793\n","Epoch 14/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 205ms/step - accuracy: 0.1337 - loss: 2.0795 - val_accuracy: 0.1728 - val_loss: 2.0794\n","Epoch 15/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 294ms/step - accuracy: 0.1034 - loss: 2.0839 - val_accuracy: 0.1235 - val_loss: 2.0798\n","Epoch 16/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 203ms/step - accuracy: 0.1167 - loss: 2.0807 - val_accuracy: 0.1235 - val_loss: 2.0796\n","Epoch 17/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 298ms/step - accuracy: 0.1358 - loss: 2.0785 - val_accuracy: 0.1235 - val_loss: 2.0795\n","Epoch 18/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 218ms/step - accuracy: 0.1248 - loss: 2.0819 - val_accuracy: 0.1173 - val_loss: 2.0795\n","Epoch 19/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - accuracy: 0.1248 - loss: 2.0821 - val_accuracy: 0.1235 - val_loss: 2.0794\n","Epoch 20/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 270ms/step - accuracy: 0.1162 - loss: 2.0778 - val_accuracy: 0.1235 - val_loss: 2.0794\n","Epoch 21/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 213ms/step - accuracy: 0.1183 - loss: 2.0794 - val_accuracy: 0.1235 - val_loss: 2.0794\n","Training efficientnet_v2_b0...\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n","\u001b[1m24274472/24274472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Epoch 1/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 1s/step - accuracy: 0.1370 - loss: 2.2337 - val_accuracy: 0.1235 - val_loss: 2.0938\n","Epoch 2/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 228ms/step - accuracy: 0.1279 - loss: 2.1779 - val_accuracy: 0.1235 - val_loss: 2.1031\n","Epoch 3/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 301ms/step - accuracy: 0.1211 - loss: 2.1340 - val_accuracy: 0.1235 - val_loss: 2.0846\n","Epoch 4/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 224ms/step - accuracy: 0.1273 - loss: 2.1098 - val_accuracy: 0.1235 - val_loss: 2.0796\n","Epoch 5/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 249ms/step - accuracy: 0.1009 - loss: 2.0829 - val_accuracy: 0.1235 - val_loss: 2.0800\n","Epoch 6/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 284ms/step - accuracy: 0.1170 - loss: 2.0899 - val_accuracy: 0.1358 - val_loss: 2.0791\n","Epoch 7/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 222ms/step - accuracy: 0.1191 - loss: 2.0851 - val_accuracy: 0.1235 - val_loss: 2.0796\n","Epoch 8/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 348ms/step - accuracy: 0.1425 - loss: 2.0840 - val_accuracy: 0.1358 - val_loss: 2.0791\n","Epoch 9/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 263ms/step - accuracy: 0.1475 - loss: 2.0762 - val_accuracy: 0.1358 - val_loss: 2.0791\n","Epoch 10/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 229ms/step - accuracy: 0.1180 - loss: 2.0820 - val_accuracy: 0.1235 - val_loss: 2.0795\n","Epoch 11/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 317ms/step - accuracy: 0.1134 - loss: 2.0827 - val_accuracy: 0.1235 - val_loss: 2.0795\n","Epoch 12/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 236ms/step - accuracy: 0.0970 - loss: 2.0808 - val_accuracy: 0.1235 - val_loss: 2.0795\n","Epoch 13/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 223ms/step - accuracy: 0.1266 - loss: 2.0806 - val_accuracy: 0.1235 - val_loss: 2.0794\n","Epoch 14/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 232ms/step - accuracy: 0.1298 - loss: 2.0763 - val_accuracy: 0.1235 - val_loss: 2.0794\n","Epoch 15/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 259ms/step - accuracy: 0.1294 - loss: 2.0833 - val_accuracy: 0.1235 - val_loss: 2.0795\n","Epoch 16/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 223ms/step - accuracy: 0.1075 - loss: 2.0820 - val_accuracy: 0.1235 - val_loss: 2.0794\n","Epoch 17/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 307ms/step - accuracy: 0.1208 - loss: 2.0821 - val_accuracy: 0.1235 - val_loss: 2.0794\n","Epoch 18/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 260ms/step - accuracy: 0.1083 - loss: 2.0795 - val_accuracy: 0.1235 - val_loss: 2.0794\n","Epoch 19/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 224ms/step - accuracy: 0.1442 - loss: 2.0826 - val_accuracy: 0.1235 - val_loss: 2.0794\n","Training resnet50...\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Epoch 1/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 815ms/step - accuracy: 0.0998 - loss: 2.4234 - val_accuracy: 0.1235 - val_loss: 2.1705\n","Epoch 2/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 341ms/step - accuracy: 0.1550 - loss: 2.1753 - val_accuracy: 0.1235 - val_loss: 2.0932\n","Epoch 3/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 288ms/step - accuracy: 0.1267 - loss: 2.1230 - val_accuracy: 0.1296 - val_loss: 2.0889\n","Epoch 4/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 260ms/step - accuracy: 0.1184 - loss: 2.1169 - val_accuracy: 0.0556 - val_loss: 2.0813\n","Epoch 5/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 331ms/step - accuracy: 0.1586 - loss: 2.0804 - val_accuracy: 0.1235 - val_loss: 2.0845\n","Epoch 6/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 284ms/step - accuracy: 0.1239 - loss: 2.0997 - val_accuracy: 0.1235 - val_loss: 2.0840\n","Epoch 7/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 256ms/step - accuracy: 0.1334 - loss: 2.0856 - val_accuracy: 0.0802 - val_loss: 2.0795\n","Epoch 8/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 413ms/step - accuracy: 0.1107 - loss: 2.0853 - val_accuracy: 0.1605 - val_loss: 2.0783\n","Epoch 9/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 259ms/step - accuracy: 0.1291 - loss: 2.0803 - val_accuracy: 0.1358 - val_loss: 2.0785\n","Epoch 10/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 261ms/step - accuracy: 0.1074 - loss: 2.0762 - val_accuracy: 0.1235 - val_loss: 2.0785\n","Epoch 11/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 352ms/step - accuracy: 0.1060 - loss: 2.0787 - val_accuracy: 0.1173 - val_loss: 2.0785\n","Epoch 12/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 255ms/step - accuracy: 0.1158 - loss: 2.0784 - val_accuracy: 0.1111 - val_loss: 2.0785\n","Epoch 13/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 273ms/step - accuracy: 0.1105 - loss: 2.0803 - val_accuracy: 0.1358 - val_loss: 2.0785\n","Epoch 14/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 349ms/step - accuracy: 0.1223 - loss: 2.0771 - val_accuracy: 0.1481 - val_loss: 2.0785\n","Epoch 15/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 256ms/step - accuracy: 0.1261 - loss: 2.0785 - val_accuracy: 0.1605 - val_loss: 2.0785\n","Epoch 16/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 307ms/step - accuracy: 0.1309 - loss: 2.0755 - val_accuracy: 0.1296 - val_loss: 2.0785\n","Epoch 17/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 270ms/step - accuracy: 0.1252 - loss: 2.0764 - val_accuracy: 0.1420 - val_loss: 2.0785\n","Epoch 18/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 259ms/step - accuracy: 0.1011 - loss: 2.0743 - val_accuracy: 0.1420 - val_loss: 2.0785\n","Training densenet121...\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Epoch 1/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 2s/step - accuracy: 0.1028 - loss: 2.3323 - val_accuracy: 0.1235 - val_loss: 2.1134\n","Epoch 2/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 354ms/step - accuracy: 0.1043 - loss: 2.1990 - val_accuracy: 0.1358 - val_loss: 2.0814\n","Epoch 3/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 264ms/step - accuracy: 0.0780 - loss: 2.1793 - val_accuracy: 0.1235 - val_loss: 2.0811\n","Epoch 4/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 331ms/step - accuracy: 0.0954 - loss: 2.1209 - val_accuracy: 0.1111 - val_loss: 2.0757\n","Epoch 5/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 363ms/step - accuracy: 0.1154 - loss: 2.1080 - val_accuracy: 0.1235 - val_loss: 2.0758\n","Epoch 6/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 310ms/step - accuracy: 0.1179 - loss: 2.1055 - val_accuracy: 0.1420 - val_loss: 2.0737\n","Epoch 7/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 263ms/step - accuracy: 0.1197 - loss: 2.0824 - val_accuracy: 0.1481 - val_loss: 2.0743\n","Epoch 8/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 343ms/step - accuracy: 0.1119 - loss: 2.0831 - val_accuracy: 0.1235 - val_loss: 2.0763\n","Epoch 9/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 269ms/step - accuracy: 0.1040 - loss: 2.0880 - val_accuracy: 0.1543 - val_loss: 2.0744\n","Epoch 10/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 261ms/step - accuracy: 0.1357 - loss: 2.0780 - val_accuracy: 0.1543 - val_loss: 2.0752\n","Epoch 11/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 317ms/step - accuracy: 0.1335 - loss: 2.0787 - val_accuracy: 0.1173 - val_loss: 2.0757\n","Epoch 12/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 288ms/step - accuracy: 0.1543 - loss: 2.0830 - val_accuracy: 0.1173 - val_loss: 2.0775\n","Epoch 13/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 258ms/step - accuracy: 0.1198 - loss: 2.0811 - val_accuracy: 0.1481 - val_loss: 2.0771\n","Epoch 14/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 351ms/step - accuracy: 0.1526 - loss: 2.0823 - val_accuracy: 0.1358 - val_loss: 2.0771\n","Epoch 15/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 253ms/step - accuracy: 0.1155 - loss: 2.0828 - val_accuracy: 0.1481 - val_loss: 2.0771\n","Epoch 16/50\n","\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 266ms/step - accuracy: 0.1149 - loss: 2.0777 - val_accuracy: 0.1296 - val_loss: 2.0770\n"]}],"source":["# Train each model and evaluate\n","best_model = None\n","best_accuracy = 0\n","history = None\n","\n","for model_name, preprocess_func in models.items():\n","    print(f'Training {model_name}...')\n","    model = build_model(model_name, preprocess_func)\n","\n","    hist = model.fit(\n","        train_datagen.flow(X_train, y_train, batch_size=32),\n","        validation_data=val_datagen.flow(X_val, y_val),\n","        epochs=50,\n","        callbacks=[early_stop, checkpoint],\n","        class_weight=class_weights,\n","        verbose=1\n","    )\n","\n","    # Evaluate on validation data\n","    val_loss, val_acc = model.evaluate(val_datagen.flow(X_val, y_val), verbose=0)\n","\n","    # Save the best model\n","    if val_acc > best_accuracy:\n","        best_accuracy = val_acc\n","        best_model = model\n","        history = hist"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":2985,"status":"error","timestamp":1729060907147,"user":{"displayName":"EmailForMScAILecsInLab","userId":"14358357769279669997"},"user_tz":-330},"id":"nX5ofHNr-PvG","outputId":"b72ef120-1cd2-4bdf-aaea-094b88b8fa74"},"outputs":[{"ename":"ValueError","evalue":"A total of 102 objects could not be loaded. Example error message for object <Conv2D name=conv1_conv, built=True>:\n\nLayer 'conv1_conv' expected 2 variables, but received 1 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Conv2D name=conv1_conv, built=True>, <Conv2D name=conv2_block1_1_conv, built=True>, <Conv2D name=conv2_block1_2_conv, built=True>, <BatchNormalization name=conv2_block1_2_bn, built=True>, <Conv2D name=conv2_block1_0_conv, built=True>, <Conv2D name=conv2_block1_3_conv, built=True>, <BatchNormalization name=conv2_block1_0_bn, built=True>, <BatchNormalization name=conv2_block1_3_bn, built=True>, <Conv2D name=conv2_block2_1_conv, built=True>, <BatchNormalization name=conv2_block2_1_bn, built=True>, <Conv2D name=conv2_block2_2_conv, built=True>, <BatchNormalization name=conv2_block2_2_bn, built=True>, <Conv2D name=conv2_block2_3_conv, built=True>, <BatchNormalization name=conv2_block2_3_bn, built=True>, <Conv2D name=conv2_block3_1_conv, built=True>, <BatchNormalization name=conv2_block3_1_bn, built=True>, <Conv2D name=conv2_block3_2_conv, built=True>, <BatchNormalization name=conv2_block3_2_bn, built=True>, <Conv2D name=conv2_block3_3_conv, built=True>, <BatchNormalization name=conv2_block3_3_bn, built=True>, <Conv2D name=conv3_block1_1_conv, built=True>, <BatchNormalization name=conv3_block1_1_bn, built=True>, <Conv2D name=conv3_block1_2_conv, built=True>, <Conv2D name=conv3_block1_0_conv, built=True>, <Conv2D name=conv3_block1_3_conv, built=True>, <BatchNormalization name=conv3_block1_0_bn, built=True>, <BatchNormalization name=conv3_block1_3_bn, built=True>, <Conv2D name=conv3_block2_1_conv, built=True>, <Conv2D name=conv3_block2_2_conv, built=True>, <BatchNormalization name=conv3_block2_2_bn, built=True>, <Conv2D name=conv3_block2_3_conv, built=True>, <BatchNormalization name=conv3_block2_3_bn, built=True>, <Conv2D name=conv3_block3_1_conv, built=True>, <BatchNormalization name=conv3_block3_1_bn, built=True>, <Conv2D name=conv3_block3_2_conv, built=True>, <Conv2D name=conv3_block3_3_conv, built=True>, <BatchNormalization name=conv3_block3_3_bn, built=True>, <Conv2D name=conv3_block4_1_conv, built=True>, <Conv2D name=conv3_block4_2_conv, built=True>, <BatchNormalization name=conv3_block4_2_bn, built=True>, <Conv2D name=conv3_block4_3_conv, built=True>, <BatchNormalization name=conv3_block4_3_bn, built=True>, <Conv2D name=conv4_block1_1_conv, built=True>, <BatchNormalization name=conv4_block1_1_bn, built=True>, <Conv2D name=conv4_block1_2_conv, built=True>, <BatchNormalization name=conv4_block1_2_bn, built=True>, <Conv2D name=conv4_block1_0_conv, built=True>, <Conv2D name=conv4_block1_3_conv, built=True>, <BatchNormalization name=conv4_block1_0_bn, built=True>, <BatchNormalization name=conv4_block1_3_bn, built=True>, <Conv2D name=conv4_block2_1_conv, built=True>, <BatchNormalization name=conv4_block2_1_bn, built=True>, <Conv2D name=conv4_block2_2_conv, built=True>, <BatchNormalization name=conv4_block2_2_bn, built=True>, <Conv2D name=conv4_block2_3_conv, built=True>, <BatchNormalization name=conv4_block2_3_bn, built=True>, <Conv2D name=conv4_block3_1_conv, built=True>, <BatchNormalization name=conv4_block3_1_bn, built=True>, <Conv2D name=conv4_block3_2_conv, built=True>, <BatchNormalization name=conv4_block3_2_bn, built=True>, <Conv2D name=conv4_block3_3_conv, built=True>, <BatchNormalization name=conv4_block3_3_bn, built=True>, <Conv2D name=conv4_block4_1_conv, built=True>, <BatchNormalization name=conv4_block4_1_bn, built=True>, <Conv2D name=conv4_block4_2_conv, built=True>, <BatchNormalization name=conv4_block4_2_bn, built=True>, <Conv2D name=conv4_block4_3_conv, built=True>, <BatchNormalization name=conv4_block4_3_bn, built=True>, <Conv2D name=conv4_block5_1_conv, built=True>, <BatchNormalization name=conv4_block5_1_bn, built=True>, <Conv2D name=conv4_block5_2_conv, built=True>, <BatchNormalization name=conv4_block5_2_bn, built=True>, <Conv2D name=conv4_block5_3_conv, built=True>, <BatchNormalization name=conv4_block5_3_bn, built=True>, <Conv2D name=conv4_block6_1_conv, built=True>, <BatchNormalization name=conv4_block6_1_bn, built=True>, <Conv2D name=conv4_block6_2_conv, built=True>, <BatchNormalization name=conv4_block6_2_bn, built=True>, <Conv2D name=conv4_block6_3_conv, built=True>, <BatchNormalization name=conv4_block6_3_bn, built=True>, <Conv2D name=conv5_block1_1_conv, built=True>, <BatchNormalization name=conv5_block1_1_bn, built=True>, <Conv2D name=conv5_block1_2_conv, built=True>, <BatchNormalization name=conv5_block1_2_bn, built=True>, <Conv2D name=conv5_block1_0_conv, built=True>, <Conv2D name=conv5_block1_3_conv, built=True>, <BatchNormalization name=conv5_block1_0_bn, built=True>, <BatchNormalization name=conv5_block1_3_bn, built=True>, <Conv2D name=conv5_block2_1_conv, built=True>, <BatchNormalization name=conv5_block2_1_bn, built=True>, <Conv2D name=conv5_block2_2_conv, built=True>, <BatchNormalization name=conv5_block2_2_bn, built=True>, <Conv2D name=conv5_block2_3_conv, built=True>, <BatchNormalization name=conv5_block2_3_bn, built=True>, <Conv2D name=conv5_block3_1_conv, built=True>, <BatchNormalization name=conv5_block3_1_bn, built=True>, <Conv2D name=conv5_block3_2_conv, built=True>, <BatchNormalization name=conv5_block3_2_bn, built=True>, <Conv2D name=conv5_block3_3_conv, built=True>, <BatchNormalization name=conv5_block3_3_bn, built=True>, <Dense name=dense_4, built=True>, <keras.src.optimizers.adam.Adam object at 0x7f995a75b760>]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-d7123024ac44>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_raise_loading_failure\u001b[0;34m(error_msgs, warn_only)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: A total of 102 objects could not be loaded. Example error message for object <Conv2D name=conv1_conv, built=True>:\n\nLayer 'conv1_conv' expected 2 variables, but received 1 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Conv2D name=conv1_conv, built=True>, <Conv2D name=conv2_block1_1_conv, built=True>, <Conv2D name=conv2_block1_2_conv, built=True>, <BatchNormalization name=conv2_block1_2_bn, built=True>, <Conv2D name=conv2_block1_0_conv, built=True>, <Conv2D name=conv2_block1_3_conv, built=True>, <BatchNormalization name=conv2_block1_0_bn, built=True>, <BatchNormalization name=conv2_block1_3_bn, built=True>, <Conv2D name=conv2_block2_1_conv, built=True>, <BatchNormalization name=conv2_block2_1_bn, built=True>, <Conv2D name=conv2_block2_2_conv, built=True>, <BatchNormalization name=conv2_block2_2_bn, built=True>, <Conv2D name=conv2_block2_3_conv, built=True>, <BatchNormalization name=conv2_block2_3_bn, built=True>, <Conv2D name=conv2_block3_1_conv, built=True>, <BatchNormalization name=conv2_block3_1_bn, built=True>, <Conv2D name=conv2_block3_2_conv, built=True>, <BatchNormalization name=conv2_block3_2_bn, built=True>, <Conv2D name=conv2_block3_3_conv, built=True>, <BatchNormalization name=conv2_block3_3_bn, built=True>, <Conv2D name=conv3_block1_1_conv, built=True>, <BatchNormalization name=conv3_block1_1_bn, built=True>, <Conv2D name=conv3_block1_2_conv, built=True>, <Conv2D name=conv3_block1_0_conv, built=True>, <Conv2D name=conv3_block1_3_conv, built=True>, <BatchNormalization name=conv3_block1_0_bn, built=True>, <BatchNormalization name=conv3_block1_3_bn, built=True>, <Conv2D name=conv3_block2_1_conv, built=True>, <Conv2D name=conv3_block2_2_conv, built=True>, <BatchNormalization na..."]}],"source":["# Load the best model\n","best_model.load_weights('best_model.keras')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0Kcw4JCwBznG","outputId":"6bb6ca25-1f94-44c6-8e84-7f620efa7a9c"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'clothing'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-da89c384febd>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'clothing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Create data loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clothing'"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader, WeightedRandomSampler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define data transforms with advanced augmentation\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomVerticalFlip(),\n","    transforms.RandomRotation(30),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# Load the dataset\n","dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Clothing_Data_HRP', transform=train_transform)\n","\n","# Calculate class weights for balanced sampling\n","class_weights = []\n","for root, subdir, files in os.walk('/content/drive/MyDrive/Clothing_Data_HRP'):\n","    if len(files) > 0:\n","        class_weights.append(1/len(files))\n","sample_weights = [0] * len(dataset)\n","for idx, (data, label) in enumerate(dataset):\n","    class_weight = class_weights[label]\n","    sample_weights[idx] = class_weight\n","\n","# Create weighted sampler\n","sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n","\n","# Split the dataset\n","train_indices, test_indices = train_test_split(list(range(len(dataset))), test_size=0.2, stratify=dataset.targets)\n","\n","train_dataset = torch.utils.data.Subset(dataset, train_indices)\n","test_dataset = torch.utils.data.Subset(datasets.ImageFolder(root='clothing', transform=test_transform), test_indices)\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Define models\n","model_functions = {\n","    'mobilenet_v3_small': models.mobilenet_v3_small,\n","    'efficientnet_v2_b0': models.efficientnet_v2_s,\n","    'resnet50': models.resnet50,\n","    'regnet_y_400mf': models.regnet_y_400mf,\n","    'densenet121': models.densenet121\n","}\n","\n","def train_model(model_name, num_classes, num_epochs=50, patience=10):\n","    model = model_functions[model_name](pretrained=True)\n","\n","    # Modify the last layer for our number of classes\n","    if model_name.startswith('mobilenet') or model_name.startswith('efficientnet'):\n","        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_classes)\n","    elif model_name.startswith('resnet') or model_name.startswith('regnet'):\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    elif model_name.startswith('densenet'):\n","        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n","\n","    model = model.to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5)\n","\n","    best_acc = 0.0\n","    best_model = None\n","    epochs_no_improve = 0\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","        train_loss = running_loss / len(train_loader)\n","        train_acc = correct / total\n","\n","        # Validation\n","        model.eval()\n","        val_correct = 0\n","        val_total = 0\n","        with torch.no_grad():\n","            for inputs, labels in test_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                _, predicted = outputs.max(1)\n","                val_total += labels.size(0)\n","                val_correct += predicted.eq(labels).sum().item()\n","\n","        val_acc = val_correct / val_total\n","        scheduler.step(val_acc)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","        if val_acc > best_acc:\n","            best_acc = val_acc\n","            best_model = model.state_dict()\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","            if epochs_no_improve == patience:\n","                print(\"Early stopping\")\n","                break\n","\n","    return best_model, best_acc\n","\n","# Train and evaluate each model\n","results = {}\n","for model_name in model_functions.keys():\n","    print(f\"Training {model_name}\")\n","    best_model, best_acc = train_model(model_name, len(dataset.classes))\n","    results[model_name] = (best_model, best_acc)\n","\n","# Find the best performing model\n","best_model_name = max(results, key=lambda k: results[k][1])\n","best_model_state, best_acc = results[best_model_name]\n","\n","print(f\"Best model: {best_model_name} with accuracy: {best_acc:.4f}\")\n","\n","# Save the best model\n","torch.save(best_model_state, f\"best_model_{best_model_name}.pth\")\n","\n","# Load the best model for evaluation\n","model = model_functions[best_model_name](pretrained=False)\n","if best_model_name.startswith('mobilenet') or best_model_name.startswith('efficientnet'):\n","    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, len(dataset.classes))\n","elif best_model_name.startswith('resnet') or best_model_name.startswith('regnet'):\n","    model.fc = nn.Linear(model.fc.in_features, len(dataset.classes))\n","elif best_model_name.startswith('densenet'):\n","    model.classifier = nn.Linear(model.classifier.in_features, len(dataset.classes))\n","\n","model.load_state_dict(best_model_state)\n","model = model.to(device)\n","model.eval()\n","\n","# Evaluation\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","# Print classification report and confusion matrix\n","print(\"\\nClassification Report:\")\n","print(classification_report(all_labels, all_preds, target_names=dataset.classes))\n","\n","print(\"\\nConfusion Matrix:\")\n","cm = confusion_matrix(all_labels, all_preds)\n","plt.figure(figsize=(10, 8))\n","plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n","plt.title(\"Confusion Matrix\")\n","plt.colorbar()\n","tick_marks = np.arange(len(dataset.classes))\n","plt.xticks(tick_marks, dataset.classes, rotation=45)\n","plt.yticks(tick_marks, dataset.classes)\n","plt.tight_layout()\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","plt.savefig('confusion_matrix.png')\n","plt.close()\n","\n","print(f\"Confusion matrix saved as 'confusion_matrix.png'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvBM2sCMOYQD"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1Kzipd9TyogggW7RvA3KcrlAI1NNufkmh","authorship_tag":"ABX9TyNWefrurm+lSFAVAEI4oOpG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}