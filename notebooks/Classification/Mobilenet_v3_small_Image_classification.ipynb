{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mobilenet_v3_small"
      ],
      "metadata": {
        "id": "aw0yXJdq6EgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, random_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "# Set device (GPU/CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data Augmentation and Normalization for Training and Validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.RandomGrayscale(p=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/clothing_data'  # Folder containing the dataset with subfolders for each class\n",
        "\n",
        "# Load the entire dataset from the folder structure\n",
        "full_dataset = datasets.ImageFolder(data_dir, transform=data_transforms['train'])\n",
        "\n",
        "# Split dataset into training (80%) and validation (20%)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Apply validation transforms to the validation dataset\n",
        "val_dataset.dataset.transform = data_transforms['val']\n",
        "\n",
        "# Calculate Class Weights for Balanced Sampling on Training Set\n",
        "train_labels = [full_dataset.targets[i] for i in train_dataset.indices]\n",
        "class_counts = Counter(train_labels)\n",
        "class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
        "sample_weights = [class_weights[train_labels[i]] for i in range(len(train_labels))]\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# Create DataLoader with Balanced Sampling for Training\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_dataset, batch_size=32, sampler=sampler, num_workers=4),\n",
        "    'val': DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "}\n",
        "\n",
        "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "class_names = full_dataset.classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Load Pretrained Model (MobileNetV3 Small)\n",
        "model = models.mobilenet_v3_small(pretrained=True)\n",
        "model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)  # Modify last layer for new num_classes\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss Function, Optimizer, and Scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Early Stopping Parameters\n",
        "early_stopping_patience = 10\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "early_stop_counter = 0\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
        "    global best_acc, best_model_wts, early_stop_counter\n",
        "    since = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()  # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + Optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # Deep copy the model if validation accuracy improves\n",
        "            if phase == 'val':\n",
        "                if epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    early_stop_counter = 0  # Reset early stopping counter\n",
        "                else:\n",
        "                    early_stop_counter += 1\n",
        "\n",
        "        print()\n",
        "\n",
        "        # Check early stopping condition\n",
        "        if early_stop_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered. Stopping training...\")\n",
        "            break\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:.4f}')\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train the Model\n",
        "model = train_model(model, criterion, optimizer, scheduler, num_epochs=50)\n",
        "\n",
        "# Save the best model\n",
        "torch.save(model.state_dict(), 'best_model_mobilenetv3.pth')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloaders['val']:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "# Evaluate the Model\n",
        "evaluate_model(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCFPihWMUz6l",
        "outputId": "f74dc2c1-5b21-4c7a-ecba-32247fb5b133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "----------\n",
            "train Loss: 1.1618 Acc: 0.5487\n",
            "val Loss: 1.1626 Acc: 0.5617\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n",
            "train Loss: 0.4389 Acc: 0.8439\n",
            "val Loss: 1.0289 Acc: 0.6605\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n",
            "train Loss: 0.3302 Acc: 0.8733\n",
            "val Loss: 1.0047 Acc: 0.7037\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n",
            "train Loss: 0.1933 Acc: 0.9382\n",
            "val Loss: 1.5582 Acc: 0.6235\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n",
            "train Loss: 0.1040 Acc: 0.9675\n",
            "val Loss: 1.1610 Acc: 0.7099\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n",
            "train Loss: 0.1019 Acc: 0.9675\n",
            "val Loss: 1.1284 Acc: 0.7099\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n",
            "train Loss: 0.1450 Acc: 0.9598\n",
            "val Loss: 1.8917 Acc: 0.6358\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n",
            "train Loss: 0.0948 Acc: 0.9722\n",
            "val Loss: 1.3727 Acc: 0.6790\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n",
            "train Loss: 0.0784 Acc: 0.9737\n",
            "val Loss: 1.0431 Acc: 0.7469\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n",
            "train Loss: 0.0420 Acc: 0.9876\n",
            "val Loss: 0.8558 Acc: 0.7778\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n",
            "train Loss: 0.0302 Acc: 0.9923\n",
            "val Loss: 0.7587 Acc: 0.7778\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n",
            "train Loss: 0.0311 Acc: 0.9938\n",
            "val Loss: 0.7088 Acc: 0.7963\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n",
            "train Loss: 0.0268 Acc: 0.9969\n",
            "val Loss: 0.6656 Acc: 0.8086\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n",
            "train Loss: 0.0196 Acc: 0.9954\n",
            "val Loss: 0.6720 Acc: 0.8086\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n",
            "train Loss: 0.0089 Acc: 1.0000\n",
            "val Loss: 0.6487 Acc: 0.8148\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n",
            "train Loss: 0.0117 Acc: 0.9985\n",
            "val Loss: 0.6391 Acc: 0.8148\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n",
            "train Loss: 0.0174 Acc: 0.9969\n",
            "val Loss: 0.6237 Acc: 0.8210\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n",
            "train Loss: 0.0141 Acc: 0.9985\n",
            "val Loss: 0.6039 Acc: 0.8210\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n",
            "train Loss: 0.0105 Acc: 0.9985\n",
            "val Loss: 0.6000 Acc: 0.8210\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n",
            "train Loss: 0.0164 Acc: 0.9954\n",
            "val Loss: 0.5956 Acc: 0.8272\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n",
            "train Loss: 0.0116 Acc: 1.0000\n",
            "val Loss: 0.5825 Acc: 0.8272\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n",
            "train Loss: 0.0124 Acc: 1.0000\n",
            "val Loss: 0.5803 Acc: 0.8210\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n",
            "train Loss: 0.0209 Acc: 0.9954\n",
            "val Loss: 0.5744 Acc: 0.8272\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n",
            "train Loss: 0.0142 Acc: 0.9969\n",
            "val Loss: 0.5745 Acc: 0.8272\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n",
            "train Loss: 0.0235 Acc: 0.9954\n",
            "val Loss: 0.5642 Acc: 0.8333\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n",
            "train Loss: 0.0134 Acc: 0.9985\n",
            "val Loss: 0.5656 Acc: 0.8210\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n",
            "train Loss: 0.0174 Acc: 0.9985\n",
            "val Loss: 0.5649 Acc: 0.8210\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n",
            "train Loss: 0.0183 Acc: 0.9954\n",
            "val Loss: 0.5694 Acc: 0.8148\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n",
            "train Loss: 0.0150 Acc: 0.9938\n",
            "val Loss: 0.5648 Acc: 0.8210\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n",
            "train Loss: 0.0099 Acc: 1.0000\n",
            "val Loss: 0.5605 Acc: 0.8272\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n",
            "train Loss: 0.0170 Acc: 0.9954\n",
            "val Loss: 0.5704 Acc: 0.8148\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n",
            "train Loss: 0.0118 Acc: 0.9985\n",
            "val Loss: 0.5783 Acc: 0.8148\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n",
            "train Loss: 0.0098 Acc: 0.9969\n",
            "val Loss: 0.5795 Acc: 0.8148\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n",
            "train Loss: 0.0155 Acc: 0.9969\n",
            "val Loss: 0.5735 Acc: 0.8086\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n",
            "train Loss: 0.0107 Acc: 0.9985\n",
            "val Loss: 0.5813 Acc: 0.8086\n",
            "\n",
            "Early stopping triggered. Stopping training...\n",
            "Training complete in 3m 49s\n",
            "Best val Acc: 0.8333\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "       female bodycon       0.90      0.93      0.91        28\n",
            " female_pant_suit_top       0.79      1.00      0.88        15\n",
            "  female_pants_bottom       0.77      0.85      0.81        20\n",
            "  informal cloths men       0.78      0.72      0.75        25\n",
            "informal cloths women       0.82      0.64      0.72        22\n",
            "           male_pants       0.81      0.81      0.81        16\n",
            "            shirt men       0.90      0.86      0.88        22\n",
            "            suits men       0.87      0.93      0.90        14\n",
            "\n",
            "             accuracy                           0.83       162\n",
            "            macro avg       0.83      0.84      0.83       162\n",
            "         weighted avg       0.83      0.83      0.83       162\n",
            "\n",
            "Confusion Matrix:\n",
            "[[26  1  0  0  1  0  0  0]\n",
            " [ 0 15  0  0  0  0  0  0]\n",
            " [ 0  0 17  0  0  3  0  0]\n",
            " [ 2  1  1 18  2  0  1  0]\n",
            " [ 1  1  1  3 14  0  1  1]\n",
            " [ 0  0  3  0  0 13  0  0]\n",
            " [ 0  0  0  2  0  0 19  1]\n",
            " [ 0  1  0  0  0  0  0 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, random_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Set device (GPU/CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data Augmentation and Normalization for Training and Validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.RandomGrayscale(p=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/clothing_data'  # Folder containing the dataset with subfolders for each class\n",
        "\n",
        "# Load the entire dataset from the folder structure\n",
        "full_dataset = datasets.ImageFolder(data_dir, transform=data_transforms['train'])\n",
        "\n",
        "# Split dataset into training (80%) and validation (20%)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Apply validation transforms to the validation dataset\n",
        "val_dataset.dataset.transform = data_transforms['val']\n",
        "\n",
        "# Calculate Class Weights for Balanced Sampling on Training Set\n",
        "train_labels = [full_dataset.targets[i] for i in train_dataset.indices]\n",
        "class_counts = Counter(train_labels)\n",
        "class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
        "sample_weights = [class_weights[train_labels[i]] for i in range(len(train_labels))]\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# Create DataLoader with Balanced Sampling for Training\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_dataset, batch_size=32, sampler=sampler, num_workers=4),\n",
        "    'val': DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "}\n",
        "\n",
        "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "class_names = full_dataset.classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Load Pretrained ResNet50 Model\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)  # Modify last layer for num_classes\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss Function, Optimizer, and Scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Early Stopping Parameters\n",
        "early_stopping_patience = 10\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "early_stop_counter = 0\n",
        "\n",
        "# Initialize variables to track accuracy and loss for plotting\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
        "    global best_acc, best_model_wts, early_stop_counter\n",
        "    since = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()  # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + Optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # Save loss and accuracy for plotting\n",
        "            if phase == 'train':\n",
        "                train_acc_history.append(epoch_acc.item())\n",
        "                train_loss_history.append(epoch_loss)\n",
        "            else:\n",
        "                val_acc_history.append(epoch_acc.item())\n",
        "                val_loss_history.append(epoch_loss)\n",
        "\n",
        "            # Deep copy the model if validation accuracy improves\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                early_stop_counter = 0  # Reset early stopping counter\n",
        "            elif phase == 'val':\n",
        "                early_stop_counter += 1\n",
        "\n",
        "        print()\n",
        "\n",
        "        # Check early stopping condition\n",
        "        if early_stop_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered. Stopping training...\")\n",
        "            break\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:.4f}')\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train the Model\n",
        "model = train_model(model, criterion, optimizer, scheduler, num_epochs=50)\n",
        "\n",
        "# Save the best model\n",
        "torch.save(model.state_dict(), 'best_model_resnet50.pth')\n",
        "\n",
        "# Plot Training and Validation Curves\n",
        "def plot_curves():\n",
        "    epochs = len(train_acc_history)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot accuracy curves\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(epochs), train_acc_history, label='Training Accuracy')\n",
        "    plt.plot(range(epochs), val_acc_history, label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss curves\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(epochs), train_loss_history, label='Training Loss')\n",
        "    plt.plot(range(epochs), val_loss_history, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Display accuracy and loss curves\n",
        "plot_curves()\n",
        "\n",
        "# Function to show images and predictions\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))  # Convert tensor image to numpy\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean  # De-normalize\n",
        "    inp = np.clip(inp, 0, 1)  # Clip to valid range\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # Pause to update plots\n",
        "\n",
        "# Evaluation Function with Image Display\n",
        "def evaluate_model(model, num_images=6):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    images_shown = 0\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloaders['val']:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Display a few images with predictions\n",
        "            if images_shown < num_images:\n",
        "                for i in range(inputs.size(0)):\n",
        "                    if images_shown >= num_images:\n",
        "                        break\n",
        "                    ax = plt.subplot(num_images // 2, 2, images_shown + 1)\n",
        "                    ax.axis('off')\n",
        "                    imshow(inputs.cpu().data[i])\n",
        "                    ax.set_title(f'Predicted: {class_names[preds[i]]}, True: {class_names[labels[i]]}')\n",
        "                    images_shown += 1\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "# Evaluate the Model and display some predictions\n",
        "evaluate_model(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "wpQsVmqwa3QM",
        "outputId": "914a61c0-e772-4b6e-9203-97f07b25bbce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 1.06 MiB is free. Process 4776 has 14.74 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 475.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-fda54a726cef>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Modify last layer for num_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Loss Function, Optimizer, and Scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1172\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1158\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     )\n\u001b[0;32m-> 1160\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1161\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 1.06 MiB is free. Process 4776 has 14.74 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 475.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dzzykAw_chCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}